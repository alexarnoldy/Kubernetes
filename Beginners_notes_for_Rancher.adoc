=== RKE deployed on VMs
* Needs Docker installed
* Deploys everything via hyperkube as containers
** Can deploy onto a single host with Docker or an existing K8s cluster (likely needs Docker as the CRE) for HA
* Easy to deploy etcd, controlplane, and worker on a single node
* Deploying the first node, and it seems adding a node to the cluster, are the same command, but with --etcd --controlplan

=== Creating the custom SES storage class


NOTE: For some reason the action of K8s trying to mount an RBD image to a pod doesn't cause the rbd kernel module to load. Use `lsmod | grep rbd` to check it and `sudo modprobe rbd` to load it non-persitently

* Need to ensure the rbd kernel module is loaded on boot: `sudo bash -c "echo rbd > /etc/modules-load.d/rbd.conf"`

.Deploy the Storage Class via the UI
* Create the secrets with the command line first. They cannot be created through the UI

CAUTION: Creating secrets through the UI seems to only create them as type=Opaque. For the RBD SC, they need to be type=kubernetes.io/rbd. If not, PVC provisioning will fail with "Cannot get secret of type kubernetes.io/rbd"

IMPORTANT: When creating secrets through Rancher UI, do not base64 encode the admin and user secrets

* Select the cluster -> Storage -> Storage Classes
* "Add Class"
* Click in the "Provisioner" field
** Enter: kubernetes.io/rbd
** Select "*kubernetes.io/rbd (Custom)*" 
* Fill the fields under Parameters
* Will only accept an imageFormat of 1 or 2
** Note that secret fields need the name of the deployed secret, not the key alone
// * Need to make sure the ceph-common package is installed on all RKE worker nodes



=== Deploying pods, aka Workloads

.Select the cluster then the namespace, aka project
* Resources -> Workloads -> Deploy
* Select Global -> <cluster name> to work on non-namespaced resources (i.e. storageClass)
* Select Global -> <cluster name> -> <namespace> to work on namespaced resources (i.e. pods)
** PVCs are found under Resources -> Workloads -> Volumes

=== Working with Istio

.Generally configuring K8s LB
* Don't need an ingress controller to specifically assign open LB IPs to a svc
* An allocated LB svc IP doesn't seem to get plumbed at the O/S level (or perhaps, more specifically, in the host network namespace)
** Doesn't respond to ping
** Can be verified with `nmap -Pn <LB IP>`
*** Seems like the public ports allocated show as open

=== Deploying Kubeflow

.After a dozen or more attempts
* Finally had success with RKE 1.15.12, Istio 1.4.10, and Kubeflow 1.0.2

NOTE: Several issues along the way make it unclear what other version combinations might work

.Storage Classes
* Didn't have success with RBD SC but likely was due to lots of other issues
* nfs-client-provisioner SC worked
* Tested through Rancher UI with an Apline pod

.Networking
* Nginx ingress disabled on deployment
* MetalLB with two (or more) IPs
** Can use the trick to create non-autoassign pool, then specify from that pool to maintain consistent IP assignment
* Test that a pod (i.e. Nginx) in the kubeflow NS can pick up an IP from MetalLB
* Enable Istio with an ingress gateway set to load balancer and specify the IP address
** Didn't try this, but could use label selectors to see that a test Nginx pod gets traffic routed through the ingress gateway

.Order of operation

.Prepare RKE nodes:
* For these tests, configured:
** A VM for the single master node: 
*** 4 vCPU, 4GB RAM, 56GB HDD (6.9GB used after KF deployment)
** Two VMs for worker nodes:
*** 4 vCPU, 4GB RAM, 16GB HDD (12GB used after KF deployment)
*** 12 vCPU, 16GB RAM, 48GB HDD (19GB used after KF deployment)
* All are on the same server and deployment performance was quite slow
* Install and enable Docker

.Deploy the Rancher UI server container (on this host or another host/VM)
* Having a LAN routable IP is helpful to be able to reach the web UI

.Prepare NFS server (used master node VM in these tests)
* Can use `sudo showmount -e` on the server to verify its serving and `sudo showmount -e <NFS server IP>` from the worker nodes to verify they have everything needed to mount

.Create a cluster through the Rancher UI
* Tried not to reuse cluster names to avoid unknown conflicts
* Verify compatability between the Kubeflow version and the K8s version: https://www.kubeflow.org/docs/started/k8s/overview/
* Disable Nginx ingress
* Enable unrestricted Pod Security Policies
* Deploy Rancher agent container with etcd and control plane on master node VM
** Wait for cluster to show green "Active"
* Deploy Rancher agent container with worker on worker node VMs
** Wait for cluster to show green "Active"
* From top menu bar, point to "Global" or the cluster name, then select the cluster name just below it
* Select the "Kubeconfig File" button
** Copy the configuration into the ${HOME}/.kube/config file on a server with kubectl installed

.Deploy nfs-client-provisioner from Helm catalog
* From top menu bar, point to "Global" or the cluster name, then point to the cluster name just below it, then select "Default" project
* From top menu bar, select "Apps", then select "Launch"
* Search for "nfs-client-provisioner", then select it
** Under "Answers", paste the following into the first "Variable" answer box:
----
nfs.server=IPAddress
nfs.path=FullyQualifiedPath
storageClass.name=nfs
storageClass.defaultClass=true
----
*** Replace "IPAddress" with the hostname or IP address of the NFS server (RKE master node in these tests)
*** Replace "FullyQualifiedPath" with the fully qualified path of the NFS share
* Select "Launch" at the bottom of the page

.Deploy MetalLB load balancer from the kubectl server
* Use MetalLB portion from https://github.com/alexarnoldy/CaaSP/blob/master/Nutanix/Nginx_Ingress_Controller_and_MetalLB.adoc (as I didn't know what answers were needed for the Rancher Apps method)
* Test deploying a pod and service into the kubeflow namespace (need to create first) that picks an IP address that is assigned to MetalLB but not in use 
* Delete the pod and service after the test has succeeded

.Enable Istio
* Ensure the cluster name is shown in the top menu bar
* Point to "Tools", then select "Istio"
* Select the appropriate version (1.4.10 for these tests)
* Under "Ingress Gateway", select "True" to enable
* Under "Select Type of...", select "LoadBalancer"
* Under "Load Balancer IP", input an IP address that is assigned to MetalLB but not in use
* Select "Save" at the bottom of the page
* Wait until Istio becomes green

.Deploy Kubeflow from the kubectl server
* Install the kfctl utility and place it in /usr/local/bin
----
wget https://github.com/kubeflow/kfctl/releases/download/v1.1.0/kfctl_v1.1.0-0-g9a3621e_linux.tar.gz
tar xvfz kfctl_v1.1.0-0-g9a3621e_linux.tar.gz 
sudo mv kfctl /usr/local/bin
kfctl version
----

* Configure the following variables
----
export KF_NAME=kubeflow-deployment
export BASE_DIR=${HOME}
export KF_DIR=${BASE_DIR}/${KF_NAME}
export CONFIG_URI="${KF_DIR}/kfctl_k8s_istio.v1.0.2.yaml"
----

* Create and enter the ~/kubeflow-deployment directory: `mkdir -p ${KF_DIR} && cd ${KF_DIR}`
* Download the kfctl.yaml config file: `wget https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.2.yaml`
* Comment out the following lines, near the top of the kfctl_k8s_istio.v1.0.2.yaml file
----
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-crds
    name: istio-crds
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-install
    name: istio-install
----

* Deploy Kubeflow: `kfctl apply -V -f ${CONFIG_URI}`

NOTE: During the first, successful test it took several hours for all of the deployments to deploy their pods. I really thought it was one of the worst failures to date, but many hours later I discovered virtually everything deployed correctly.

IMPORTANT: On every attempt at least one pod had not deployed correctly. If there are only a few, or less, Navigate to "Workloads" in the "Default Project" and delete one, wait for it to re-deploy correctly, then move on to the next one. It can take several minutes for each pod to finish re-deploying correctly.
