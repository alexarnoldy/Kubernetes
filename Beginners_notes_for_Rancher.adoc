=== RKE deployed on VMs
* Needs Docker installed
* Deploys everything via hyperkube as containers
** Can deploy onto a single host with Docker or an existing K8s cluster (likely needs Docker as the CRE) for HA
* Easy to deploy etcd, controlplane, and worker on a single node
* Deploying the first node, and it seems adding a node to the cluster, are the same command, but with --etcd --controlplan

=== Creating the custom SES storage class


NOTE: For some reason the action of K8s trying to mount an RBD image to a pod doesn't cause the rbd kernel module to load. Use `lsmod | grep rbd` to check it and `sudo modprobe rbd` to load it non-persitently

* Need to ensure the rbd kernel module is loaded on boot: `sudo bash -c "echo rbd > /etc/modules-load.d/rbd.conf"`

.Deploy the Storage Class via the UI
* Create the secrets with the command line first. They cannot be created through the UI

CAUTION: Creating secrets through the UI seems to only create them as type=Opaque. For the RBD SC, they need to be type=kubernetes.io/rbd. If not, PVC provisioning will fail with "Cannot get secret of type kubernetes.io/rbd"

IMPORTANT: When creating secrets through Rancher UI, do not base64 encode the admin and user secrets

* Select the cluster -> Storage -> Storage Classes
* "Add Class"
* Click in the "Provisioner" field
** Enter: kubernetes.io/rbd
** Select "*kubernetes.io/rbd (Custom)*" 
* Fill the fields under Parameters
* Will only accept an imageFormat of 1 or 2
** Note that secret fields need the name of the deployed secret, not the key alone
// * Need to make sure the ceph-common package is installed on all RKE worker nodes



=== Deploying pods, aka Workloads

.Select the cluster then the namespace, aka project
* Resources -> Workloads -> Deploy
* Select Global -> <cluster name> to work on non-namespaced resources (i.e. storageClass)
* Select Global -> <cluster name> -> <namespace> to work on namespaced resources (i.e. pods)
** PVCs are found under Resources -> Workloads -> Volumes

=== Working with Istio

.Generally configuring K8s LB
* Don't need an ingress controller to specifically assign open LB IPs to a svc
* An allocated LB svc IP doesn't seem to get plumbed at the O/S level (or perhaps, more specifically, in the host network namespace)
** Doesn't respond to ping
** Can be verified with `nmap -Pn <LB IP>`
*** Seems like the public ports allocated show as open

=== Deploying Kubeflow

.After a dozen or more attempts
* Finally had success with RKE 1.15.12, Istio 1.4.10, and Kubeflow 1.0.2

NOTE: Several issues along the way make it unclear what other version combinations might work

.Storage Classes
* Didn't have success with RBD SC but likely was due to lots of other issues
* nfs-client-provisioner SC worked
* Tested through Rancher UI with an Apline pod

.Networking
* Nginx ingress disabled on deployment
* MetalLB with two (or more) IPs
** Can use the trick to create non-autoassign pool, then specify from that pool to maintain consistent IP assignment
* Test that a pod (i.e. Nginx) in the kubeflow NS can pick up an IP from MetalLB
* Enable Istio with an ingress gateway set to load balancer and specify the IP address
** Didn't try this, but could use label selectors to see that a test Nginx pod gets traffic routed through the ingress gateway

.Order of operation

.Prepare RKE nodes:
* For these tests, configured:
** A VM for the single master node: 
*** 4 vCPU, 4GB RAM, 56GB HDD (6.9GB used after KF deployment)
** Two VMs for worker nodes:
*** 4 vCPU, 4GB RAM, 16GB HDD (12GB used after KF deployment)
*** 12 vCPU, 16GB RAM, 48GB HDD (19GB used after KF deployment)
* All are on the same server and deployment performance was quite slow
* Install and enable Docker

.Deploy the Rancher UI server container (on this host or another host/VM)
* Having a LAN routable IP is helpful to be able to reach the web UI

.Prepare NFS server (used master node VM in these tests)
* Can use `sudo showmount -e` on the server to verify its serving and `sudo showmount -e <NFS server IP>` from the worker nodes to verify they have everything needed to mount

.Create a cluster through the Rancher UI
* Tried not to reuse cluster names to avoid unknown conflicts
* Verify compatability between the Kubeflow version and the K8s version: https://www.kubeflow.org/docs/started/k8s/overview/
* Disable Nginx ingress
* Enable unrestricted Pod Security Policies
* Deploy Rancher agent container with etcd and control plane on master node VM
** Wait for cluster to show green "Active"
* Deploy Rancher agent container with worker on worker node VMs
** Wait for cluster to show green "Active"
* From top menu bar, point to "Global" or the cluster name, then select the cluster name just below it
* Select the "Kubeconfig File" button
** Copy the configuration into the ${HOME}/.kube/config file on a server with kubectl installed

.Deploy nfs-client-provisioner from Helm catalog
* From top menu bar, point to "Global" or the cluster name, then point to the cluster name just below it, then select "Default" project
* From top menu bar, select "Apps", then select "Launch"
* Search for "nfs-client-provisioner", then select it
** Under "Answers", paste the following into the first "Variable" answer box:
----
nfs.server=IPAddress
nfs.path=FullyQualifiedPath
storageClass.name=nfs
storageClass.defaultClass=true
----
*** Replace "IPAddress" with the hostname or IP address of the NFS server (RKE master node in these tests)
*** Replace "FullyQualifiedPath" with the fully qualified path of the NFS share
* Select "Launch" at the bottom of the page

.Deploy MetalLB load balancer from the kubectl server
* Use MetalLB portion from https://github.com/alexarnoldy/CaaSP/blob/master/Nutanix/Nginx_Ingress_Controller_and_MetalLB.adoc (as I didn't know what answers were needed for the Rancher Apps method)

NOTE: It can be useful to configure MetalLB with at least one IP address that will not be auto-assigned and then specify that IP address for a critical service that should not be allowed to lose its external IP address to external DNS mapping.

* Test deploying a pod and service into the kubeflow namespace that picks an IP address from MetalLB (must have at least one IP not in use)
** Create the kubeflow namespace: `kubectl create ns kubeflow`
** Create the manifest for an nginx pod and load balancer service:
----
cat <<EOF> nginx-metallb-test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
## START: Default StorageClass PVC test
## To disable testing PVC creation via the default StorageClass comment 
## out all lines from here through "## END: Default StorageClass PVC test"
        volumeMounts:
        - mountPath: /mnt/rbdvol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc

---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

## END: Default StorageClass PVC test

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
----

NOTE: This will also test that a PVC can be created and attached to a pod by way of the default storage class. If this is not desired, comment out the appropriate lines, as described in the file.

** Create the pod and service: `kubectl apply -f nginx-metallb-test.yaml -n kubeflow`
** Test the service is reachable through the load balancer IP address from outside the cluster:
----
IPAddr=$(kubectl get svc -n kubeflow | grep -w nginx | awk '{print$4":"$5}' | awk -F: '{print$1":"$2}')
curl http://${IPAddr}
----
*** An HTML encode output should be displayed that includes the phrase "Thank you for using nginx."

** When finished with testing, delete the pod and service: `kubectl apply -f nginx-metallb-test.yaml -n kubeflow`

.Enable Istio
* Ensure the cluster name is shown in the top menu bar
* Point to "Tools", then select "Istio"
* Select the appropriate version (1.4.10 for these tests)
* Under "Ingress Gateway", select "True" to enable
* Under "Select Type of...", select "LoadBalancer"
* Leave "Load Balancer IP" empty to allow MetalLB to assign an IP address
** (Optionally) Provide an IP address that is assigned to MetalLB but not in use

NOTE: It can be useful to configure MetalLB with at least one IP address that will not be auto-assigned and then specify that IP address for a critical service that should not be allowed to lose its external IP address to external DNS mapping.

* Select "Save" at the bottom of the page
* Wait until Istio becomes green
* Validate the istio-ingressgateway has received an IP address: `svc -A | egrep --color 'EXTERNAL-IP|LoadBalancer'`
** (Optionally) Validate an external connection to an internal Istio service: 
*** Use the curl command to connect to a few of the *PORT(S)* listed for the istio-ingressgateway, i.e. `curl http://{$IPADDR}:15020`
*** At least one of the ports should return "404 page not found"

.Deploy Kubeflow from the kubectl server
* Install the kfctl utility and place it in /usr/local/bin
----
wget https://github.com/kubeflow/kfctl/releases/download/v1.1.0/kfctl_v1.1.0-0-g9a3621e_linux.tar.gz
tar xvfz kfctl_v1.1.0-0-g9a3621e_linux.tar.gz 
sudo mv kfctl /usr/local/bin
kfctl version
----

* Configure the following variables
----
export KF_NAME=kubeflow-deployment
export BASE_DIR=${HOME}
export KF_DIR=${BASE_DIR}/${KF_NAME}
export CONFIG_URI="${KF_DIR}/kfctl_k8s_istio.v1.0.2.yaml"
----

* Create and enter the ~/kubeflow-deployment directory: `mkdir -p ${KF_DIR} && cd ${KF_DIR}`
* Download the kfctl.yaml config file: `wget https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.2.yaml`
* Comment out the following lines, near the top of the kfctl_k8s_istio.v1.0.2.yaml file
----
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-crds
    name: istio-crds
  - kustomizeConfig:
      parameters:
      - name: namespace
        value: istio-system
      repoRef:
        name: manifests
        path: istio/istio-install
    name: istio-install
----

* Deploy Kubeflow: `kfctl apply -V -f ${CONFIG_URI}`

NOTE: During the first, successful test it took several hours for all of the deployments to deploy their pods. I really thought it was one of the worst failures to date, but many hours later I discovered virtually everything deployed correctly.

IMPORTANT: On every attempt at least one pod had not deployed correctly. If there are only a few, or less, Navigate to "Workloads" in the "Default Project" and delete one, wait for it to re-deploy correctly, then move on to the next one. It can take several minutes for each pod to finish re-deploying correctly.
